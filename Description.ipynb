{
 "metadata": {
  "name": "",
  "signature": "sha256:0468ab336ecbeff974f5ed21e27c9ceeb19a74caaa03ad5dfb2037ea0f1b8a79"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div align=\"center\">\n",
      "<h1>Data, the Humanist's New Best Friend<br/>*Course Description*</h1>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Course Supervisor**: Juan Luis Su\u00e1rez<br/>\n",
      "**E-mail**: [jsuarez@uwo.ca](mailto:jsuarez@uwo.ca)<br/>\n",
      "**Instructor**: Javier de la Rosa<br/>\n",
      "**E-mail**: [jdelaro@uwo.ca](mailto:jdelaro@uwo.ca)<br/>\n",
      "**Office**: UC 114<br/>\n",
      "**Office Hours**: Mondays 3:00pm-5:00pm<br/>\n",
      "**Meets**: Fall 2014, Mondays 12:30pm-2:30pm, Wednesdays 12:30pm-2:30pm<br/>\n",
      "**Room**: AB 148<br/>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Description"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This course is a hands-on and pragmatic introduction to computer tools and theoretical aspects of the new use of data by humanists of different disciplines. Furthermore, it will serve as an introduction to the techniques and methods used today to make sense of data from a Humanities point of view.\n",
      "\n",
      "In that sense, *Data, the Humanist's New Best Friend* is divided into three blocks (plus one extra block that covers a programming review):\n",
      "\n",
      "- Data Mining, explaining the past and predicting the future by means of data analysis.\n",
      "- Text Analysis, producing valuable information from text sources.\n",
      "- Networks Science, understanding complex structures by analyzing the relationships among their entities."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Justification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We find computers and software in almost any field of study, from STEM disciplines (Science, Technology, Engineering and Mathematics), to Education and Arts. In the Humanities, for example, there is even a whole sub-discipline (or a discipline by itself, depending on who you ask) that tries to find answers to new and old questions by digital means. This approach, referred to as Digital Humanities, usually borrows methods from the sciences to analyze their own data, and produce interpretations and conclusions.\n",
      "\n",
      "Data is today the new currency: is easy to produce and easy to collect; although the analysis still remains as one of the most complex steps in the workflow of any analytical proccess. This tendency of focusing solely into the data as the Holy Grail that solves everything, yet arguable, is producing important advances in the theoretical framework of the Data Science. Which is later translated into methods and tools with the enough madurity to be extrapolated and used in other fields. One of those is the Digital Humanities.\n",
      "\n",
      "Being able to manipulate, manage and get value out of data is not only an in demand skill for researchers, but also a future-proof recipe for digital humanists in a field that is getting familiar with fast-paced introduction of innovative products to manage data.\n",
      "\n",
      "Unfortunately, in the Humanities, data may come in a variety of formats: from mere CSV files or tables, to books, blog posts, tweets, or even network data from social or literary networks. Obviously, critical thinking and content related courses are completely necessary, but if we do not teach courses on new tools and methods, our next generation of students will graduate with limited knowledge of what can be done and how. It is not only about tools, software or applications, but, rather, it is to give students the skills they need to adapt to the changing environment of research. We do not know if Python will be widely used in 5 or 10 years, but we have to prepare our students to go beyond the next trend and apply their knowledge to the newest and coolest tool.\n",
      "\n",
      "This course is focused in three main areas that I have surmised as the most important: data mining, text analysis, and network science. The election is not casual, is based on two principles:\n",
      "\n",
      "* Most common topics in conferences such as Digital Humanities (the biggest world conference in the field), Canadian Society of Digital Humanities, or the Summer Institute Innitiative.\n",
      "* More demanded skills in Digital Humanities job postings."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Philosophy of Teaching"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I have a strong background in Computer Sciences. When I first enrolled in Computer Engineering I was very disappointed because I did not see a computer until three months into my program. All my colleagues felt the same way. And when we finally were in front of a computer, we were limited to only what the teacher wanted us to do. My first years in college were so frustrating for a guy who wanted to be roboticist, with only studying mathematics, physics and more mathematics. After finishing my Bachelor\u2019s and Master\u2019s degrees, I decided that, if I ever had the chance to teach a computer science related course, that would be intended as a learning by doing course.\n",
      "\n",
      "After several years, I understood the point of acquiring all the mathematical knowledge they had taught us, but even so, I found that it is not the best way to teach computer sciences. People need participate and experience through trial and error. For this reason I believe the teacher must play the role of a \"facilitator\" of learning, and not the role of the \"expert\" that only gives information in one direction. I am sure that teaching is an amazing and practical way of learning for teachers, and, therefore, there must be a proper environment in the class for students to freely share and express their ideas.\n",
      "\n",
      "Personally, I believe that one successful way to reach students is by \u201cspeaking\u201d their own language. Keeping myself up to date with all the trends and technologies they usually use, may mean students feel more comfortable and confident in class. On the other hand, making the content more appealing, as well as challenging is a plus, so as to catch the interest and attention of the students.\n",
      "\n",
      "Positive attitude is a must have for a teacher, and passion for what she or he teaches is very important as well. The process of teaching, interestingly, is in itself an amazing method of learning for an educator. For that reason, there must be an appropriate educational environment for students to freely share and express their ideas. The teacher is there to encourage the procurement of knowledge for the student, but they are also responsible for the instruction that comes from these exchanges. Daily small activities are intended to achieve this, promoting pair or group work whenever possible."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Methodology"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As stated previously, the content is divided into 1+3 blocks of content: a programming review to get the hands on Python and the IPython Notebook, including the setup and in-class activities; data mining and analysis,  with activities and one assignment; text analysis and processing, with activities and one assignment; and graphs and networks analysis, also with activities and one assignment.\n",
      "\n",
      "The course will employ the format of micro-lectures, as it has been [demonstrated](http://www.sciencemag.org/content/332/6031/862.full?ijkey=GMW4zTHNMM1Tc&keytype=ref&siteid=sci) that students from classes using this format outperform those from traditional lecture classes. The students are also report being happier and more engaged. Classes will be split to two hours on Mondays, and two more on Wednesdays. Officially Wednesdays are one class hour plus one lab hour, but since class time is lab time too, there will be no change in room as we will occupy a computer lab.\n",
      "\n",
      "All micro-lectures will be delivered in the form of interactive, downloadable, and executable IPython Notebooks, made available on the [course website](http://nbviewer.ipython.org/github/versae/data_for_humanists/). Students must come to class prepared as they are expected to take an active part in the lecture and activities. After teacher explained concepts from the recommended readings, and in order to ensure active participation in class, students will be assigned activities during the class to be solved, either in small groups or in pairs, in a short period of time. While students are working in their activities, teacher will help with doubts, and finally will show the answer to the class.\n",
      "\n",
      "Moreover, after each of the main blocks of contents, students will have to complete assignments to put into practice concepts learned in class.  Overall, this course is a highly interactive course that will allow students to actively learn by doing.\n",
      "\n",
      "The use of electronic devices is highly recommended, as long as they can be used to run and experiment with the examples and activities from the micro-lectures."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classes. Attendance (10%) and Participation (15%)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notebooks with the content, related exercises and the readings, will be available for the students on the [course website](http://nbviewer.ipython.org/github/versae/data_for_humanists/). The readings in the course calendar are suggested to aid the student, but they are not required to understand the lectures (although they are good research material). Every day, to promote active learning, small activities will be proposed for the students to solve, when students are supposed to work in pairs or small groups. For this reason the room should be a laboratory, as some students may not have a laptop.\n",
      "\n",
      "After finalizing each block, an assignment that covers as many concepts of the lectures as possible is proposed. Then students have more than one week to complete it, individually or in pairs."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "IPython Notebooks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Goal***: Deliver the content in an interactive way, but according to the needs and interests of the students, while keeping students engaged.\n",
      "\n",
      "In order to make the lectures more practical, the notebooks are pieces of code that run in the browser, so students can interact with them. Notebooks might be long. The idea behind having long Notebooks is to cover as much content as possible, so depending on the interest and needs of the students, lectures can go deep in one direction or another. In this way, we can have students more engaged with the lectures at the time that they actively participate in the class by exposing their interests."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Activities"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***Goal***: Guarantee that students are understanding the lectures.\n",
      "\n",
      "Activities will be proposed daily, usually several times a day. These activities are conceived to put into practice the concepts of the lecture at the time it is explained. These activities will be evaluated only as participation. All activities, after students are given time to solve them, will be explained to the class by the teacher or by volunteers. The last of the activities may be left as a home activity, and be solved by the next class day."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Assignments (36%)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There will be three different assignments based on the three main practical blocks of content (excluding review), each of which will cover a real world case using Humanities data. All assignments need to be written as IPython Notebooks. Each assignment must have 5 different parts, evaluated as follows: getting and cleaning the data (2%), summarizing the data and extracting relevant information (2%), visualizing the information (2%), and stating the conclussions (2%). Furthermore, there is two more aspects to take into account: presentation (2%), and correctness, functionality and adequacy of code (2%). All assignments will be marked from 0 (not sent) to 100.\n",
      "\n",
      "* **Data Mining (12%)**. This assignment aims to validate the acquired knowledge of the student in terms of data wrangling, converting or mapping from one *raw* form into another format that allows for more convenient consumption of the data with the help of the libraries and methods explained during the class. This will include further data visualization, data aggregation, and a statistical measures. The assignment will include a Notebook template that students must fill in. In the template, there will be excerpts of code already done and others to be filled in. The dataset to be used will be also included, although students can use their own data sets if that is their aim. A proposal for this assignment is already accessible as the Notebook [Assignment 1](../Assignment1.ipynb).\n",
      "  \n",
      "  \n",
      "* **Text Analysis (12%)**. In this assignment the student is expected to extract information and run different analysis over the text *[Moby Dick; or The Whale](http://www.gutenberg.org/cache/epub/2701/pg2701.txt)*, by Herman Melville, from [Project Gutenberg](http://www.gutenberg.org/), in order to come to conclusions without reading the book. Other text sources, web sources or corpora can be used by the student after teacher approval. The template for the assignment will include guidelines and functions prototypes for the student to run measures such as frequency distributions, word and letter counts, richness, average length per sentence and paragraph, entity extraction and grammatical structures. Visualizations are also required. Conclusions for this assignment needs to address the topic of the book and the writing style of the author for the book. The proposal will be aviable as the Notebook Assignment 2.\n",
      "\n",
      "\n",
      "* **Network Science (12%)**. Unlike previous assignments, the dataset is not given. Instead, the student is expected to use a graph of her or him choice from [Koblenz University repositories](http://konect.uni-koblenz.de/networks/). The graph can also be created by the student, although a minimum of 100 nodes and 300 relationships are mandatory in both cases. Smaller but more dense graphs can used upon aprroval by the teacher. The template for this assignment will be made availabel as the Notebook Assignment 3. It will include some guides on the expected analysis to run (degree, density, diameter, betweenness, average shortest path, closeness, clustering, modularity, and page rank), but this time there will not be code, and almost everything will need to be written by the student."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Final Project (39%)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the final project, the student must choose a problem, phenomenon, dataset or topic of interest for him or her, and use at least two of the three blocks to write a Notebook about it. Therefore, all final projects must include at least one of these tuples of contents: Data Mining and Text Analysis, Text Analysis and Network Science, or Data Mining and Network Science. Ideally, projects should include aspects from all the main blocks, since Data Mining introduces several transversal concepts.\n",
      "\n",
      "* **Proposal (5%)**.\n",
      " - One page, single space (Times New Roman or Georgia, 12pt).\n",
      " - Extensive description of the blocks of content chosen and reasons.\n",
      " - Description of the topic and motivations.\n",
      " - Description of the intended approach and methodology.\n",
      " - Tentative bibliography.\n",
      "\n",
      "* **Notebook (20%)**\n",
      " - Description of the problem to solve and motivations (2%).\n",
      " - Analysis of the problem and possible ways to approach it (2%).\n",
      " - Methodology and theoretical framework (2%).\n",
      " - Description of the approach or solution and tools used (10%).\n",
      " - Conclusions (4%).\n",
      "\n",
      "* **Oral presentation (14%)**\n",
      " - Clarity of the explanations and tools used (5%).\n",
      " - Support material (4%).\n",
      " - Conclusions and questions (5%).\n",
      "\n",
      "There is no minimal extension for the Notebook, as long as the project covers all the aspects. A small  bibliography is mandatory (APA, MLA, etc, but consistent). Deadline is **December $3^{th}$**. This is due after the oral presentations so the students may have the chance to improve their work from the comments and feedback received during the presentation. A template Notebook Project will be provided.\n",
      "\n",
      "Final Project will be marked from 0 (not sent) to 100."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Attendance**: 10% (more than 3 unjustified absences means a zero)\n",
      "* **Participation**: 15% (including exercises sent at the end of the lectures)\n",
      "* **Assignments**: 36% (3 assignments, 12% each)\n",
      "* **Final Project**: 39% (5% Proposal, 20% Essay, 14% Oral Presentation)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Course Plan"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Block 0: What is This Course About?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "[Class 01](../Class01.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This class will present the class, methodology and the IPython Notebook environment.\n",
      "\n",
      "* Goal:\n",
      " * Get the student familiar with the environment used to deliver the classes.\n",
      "* Topics:\n",
      " * Initial setup.\n",
      " * IPython.\n",
      "* Recommended readings:\n",
      " * Bret Victor. [Learnable Programming. Designing a programming system for understanding programs](http://worrydream.com/LearnableProgramming). March 2014.\n",
      "* Activity example:\n",
      " * Write the `Hello World!` Python program."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Block 1: How to Think Like a Computer Scientist (Review)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "[Class 02](../Class02.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This class will cover the basic syntax for building Python programs.\n",
      "\n",
      "* Goal:\n",
      " * Be able to write some code to solve small and not very ambitious problems\n",
      "* Topics:\n",
      " * Python syntax\n",
      " * Variables and values\n",
      " * Statements and expressions\n",
      "* Recommended readings:\n",
      "   - [Chapter 1 of *How to Think Like a Computer Scientist*](http://openbookproject.net/thinkcs/python/english2e/ch01.html)\n",
      "   - [Sections 2.1-2.9 of chapter 2 of *How to Think Like a Computer Scientist*](http://openbookproject.net/thinkcs/python/english2e/ch02.html)\n",
      "* Activity example:\n",
      " * Write a Python program that asks the user for a number and prints it back."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "[Class 03](../Class03.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This class will introduce basic programming abstractions called functions.\n",
      "\n",
      "* Goals:\n",
      " * Write code that relies on conditional actions as well as getting data from user input.\n",
      " * Encapsulate code.\n",
      "* Topics:\n",
      " * Boolean logic\n",
      " * Define functions\n",
      " * Debug problems\n",
      " * Get data from the keyboard\n",
      "* Recommended readings:\n",
      "   - [Rest of chapter 2 of *How to Think Like a Computer Scientist*](http://openbookproject.net/thinkcs/python/english2e/ch02.html)\n",
      "   - [Chapter 3 of *How to Think Like a Computer Scientist*](http://openbookproject.net/thinkcs/python/english2e/ch03.html)\n",
      "   - [Chapter 4 (up to section 4.10) of *How to Think Like a Computer Scientist*](http://openbookproject.net/thinkcs/python/english2e/ch04.html)\n",
      "* Activity example:\n",
      " * Write a function, `grade(number)`, that receives an number between 0.0 and 100.0 and returns the proper grading following the [Smith College numerology](http://cs.smith.edu/~orourke/Grading.html). For values outside the range, just print \"N/A\". For example, `grade(88.75)` returns `B+`."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "[Class 04](../Class04.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This class completes the basic knowledge of Python.\n",
      "\n",
      "* Goal:\n",
      " * Understand the importance of reutilization of code.\n",
      " * Use the adequate data type for the problem.\n",
      " * Be able to document code.\n",
      "* Topics:\n",
      " * Import and reuse code\n",
      " * Format, comment and document your code\n",
      " * Control flow execution\n",
      " * Complex data types like lists and dictionaries\n",
      "* Recommended readings:\n",
      "   - [Chapter 5 (up to section 5.8) of *How to Think Like a Computer Scientist*](http://openbookproject.net/thinkcs/python/english2e/ch05.html)\n",
      "   - [Sections 6.1-6.3, 6.5-6.6, and 6.9-6.13 of chapter 5 of *How to Think Like a Computer Scientist*](http://openbookproject.net/thinkcs/python/english2e/ch06.html)\n",
      "   - [Chapter 7 of *How to Think Like a Computer Scientist*](http://openbookproject.net/thinkcs/python/english2e/ch07.html)\n",
      "   - [Chapter 9 of *How to Think Like a Computer Scientist*](http://openbookproject.net/thinkcs/python/english2e/ch09.html)\n",
      "   - [Chapter 12 of *How to Think Like a Computer Scientist*](http://openbookproject.net/thinkcs/python/english2e/ch12.html)\n",
      "* Activity example:\n",
      " * Write a function, `freq_dict(string)`, that counts the number of aparitions of each letter in the string `string`, and returns a dictionary with letters as keys and their frequencies as values. For example, if we call `freq_dict(\"Mississippi\")`, the result must be `{'M': 1, 's': 4, 'p': 2, 'i': 4}`."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Block 2: Dealing with Data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "[Class 05](../Class05.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This class will introduce new data types and show how to read and write from and to a file.\n",
      "\n",
      "* Goals:\n",
      " * Handle basic file input and ouput operations.\n",
      " * Understand the need for more complex data types.\n",
      "* Topics:\n",
      " * Getting data into and out of Python\n",
      " * Objects\n",
      " * NumPy\n",
      " * Arrays and Matrices\n",
      " * SciPy\n",
      "* Recommended readings:\n",
      " * Section [1.3. NumPy: creating and manipulating numerical data, from Scipy lecture notes](http://scipy-lectures.github.io/intro/numpy/index.html).\n",
      " * Section [1.5.6. Statistics and random numbers: scipy.stats, from Scipy lecture notes](http://scipy-lectures.github.io/intro/scipy.html#statistics-and-random-numbers-scipy-stats).\n",
      "* Activity example:\n",
      " * The data in [`populations.txt`](files/data/populations.txt) describes the populations of hares and lynxes (and carrots) in northern Canada during 20 years. Computes and print, based on the data in `populations.txt`:\n",
      "   1. The mean and standard deviation of the populations of each species for the years in the period.\n",
      "   2. Which year each species had the largest population?\n",
      "   3. Which species has the largest population for each year. (*Hint*: argsort & fancy indexing of `np.array(['H', 'L', 'C'])`).\n",
      "   4. Which years any of the populations is above 50000. (*Hint*: comparisons and `np.any`).\n",
      "   5. The top 2 years for each species when they had the lowest populations. (*Hint*: `argsort`, fancy indexing).\n",
      "\n",
      "  ... all without for-loops."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "[Class 06](../Class06.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics:\n",
      " * Pandas\n",
      " * Cleaning data\n",
      " * Summary statistics\n",
      " * Indexing\n",
      " * Merging, joining\n",
      " * Group by\n",
      "* Recommended readings:\n",
      " * [10 Minutes to Pandas, from pydata](http://pandas.pydata.org/pandas-docs/stable/10min.html).\n",
      " * [Series, from pydata](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#series).\n",
      " * [DataFrame, from pydata](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe).\n",
      " * [Chapters 1-5, from Julia Evans pandas-cookbok](https://github.com/jvns/pandas-cookbook).\n",
      "* Activty example:\n",
      " * Given the `arts` data frame, do the next:\n",
      "   1. Clean the dates so you only see numbers.\n",
      "   2. Get the average execution year per artist.\n",
      "   3. Get the average execution year per category.\n",
      "   4. Get the number of artworks per artist. Which artist is the most prolific?\n",
      "   5. Get the number of artworks per category. Which category has the highest number?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 07"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics:\n",
      " * Split-Apply-Combine\n",
      " * Cross-tabulation and pivoting\n",
      " * String manipulation\n",
      " * matplotlib\n",
      " * Visualizations\n",
      " * Histograms\n",
      "* Recommended readings:\n",
      " * [The Split-Apply-Combine Strategy for Data Analysis](http://www.jstatsoft.org/v40/i01/paper)\n",
      " * Section [String Manipulation, 205-212, from Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do).\n",
      " * Section [Pivot Tables and Cross-Tabulation, 275-278, from Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do).\n",
      " * [Section 1.4. Matplotlib: plotting, from Scipy lecture notes](http://scipy-lectures.github.io/intro/matplotlib/matplotlib.html)\n",
      "* Activity example:\n",
      " * Given the `arts` data frame, try to do the next:\n",
      "   1. Clean the dates so you only see numbers by using string manipulations.\n",
      "   2. Plot a histogram of number of artworks per year.\n",
      "   3. Split the latest into categories."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 08"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics:\n",
      " * Statistical modeling\n",
      " * statmodels\n",
      " * Correlation\n",
      " * Regression\n",
      " * Distributions\n",
      "* Recommended readings:\n",
      " * [Chapters 7-9 and 13-14, from Statistics for Humanities](http://statisticsforhumanities.net/).\n",
      " * [Ordinary Least Squares in Python, from datarobot](http://www.datarobot.com/blog/ordinary-least-squares-in-python/).\n",
      "* Activity example:\n",
      " * Given the `arts` data frame, try to do the next:\n",
      "   1. Is there any correlation between the periods of production and the number of artworks per artist? If so, what kind?\n",
      "   2. The execution years, what kind of distribution they follow?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 09."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics:\n",
      " * Machine Learning\n",
      " * scikit-learn\n",
      " * Supervised learning: kNN\n",
      " * Unsupervised learning: K-means\n",
      " * Cross-validation\n",
      "* Recommended readings:\n",
      " * [Chapter 1, from Introduction to Machine Learning](alex.smola.org/drafts/thebook.pdf\u200e).\n",
      " * [Chapters 1-3, from A Course in Machine Learning](http://ciml.info/).\n",
      " * [Supervised learning: predicting an output variable from high-dimensional observations](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html).\n",
      " * [Unsupervised learning: seeking representations of the data, from scikit-learn](http://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html).\n",
      "* Activity example:\n",
      " * Given the `arts` data frame, try to do the next:\n",
      "   1. Train a classifier so we can predict the artist name, given the title of the artworks.\n",
      "   2. Train a classifier so we can predict the category, given the title of the artworks."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 10."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* *Work on Assignment 1*."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Block 3: Text Analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "[Class 11](../Class11.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics:\n",
      " * NLTK\n",
      " * Tokenization\n",
      " * Concordance\n",
      " * Co-Occurrence and similarity\n",
      " * Word and phrase frequencies\n",
      " * Dispersion plots\n",
      " * TextBlob\n",
      "* Deadlines:\n",
      "  * Assignment 1.\n",
      "* Recommended readings:\n",
      " * [Chapter 1, from Natural Language Processing with Python](http://www.nltk.org/book3/).\n",
      "* Activity example:\n",
      " * Create a function, `most_commont(text, n)`, that receives a list of words or a `Text` and a number and returns the most common words.\n",
      "  For example, `most_commont(moby_dick, 5)` should return the 5 most common words: `[',', 'the', '.', 'of', 'and']`."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "[Class 12](../Class12.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topcics:\n",
      " * Corpora\n",
      " * Conditional Frequency Distributions\n",
      " * Sources of data\n",
      " * Language detection\n",
      " * Machine translation\n",
      "* Recommended readings:\n",
      " * [Chapters 2-3, from Natural Language Processing with Python](http://www.nltk.org/book3/).\n",
      "* Activity example:\n",
      " * Write a program that loads feeds from the [Spanish Blog in Digital Humanities](http://humanidadesdigitales.net/blog/feed/), get the first 10 entries using `feedparser`, and for each, returns the next in English and withouth stopwords (*Hint*: take a look to the stopwords in NLTK under `nltk.corpus.stopwords.words('spanish')`):\n",
      "   - Title\n",
      "   - Number of sentences\n",
      "   - Number of words\n",
      "   - Number of unique words (vocabulary)\n",
      "   - Number of hapaxes\n",
      "   - Top 10 most frequent words"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 13"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics:\n",
      " * Regular expressions\n",
      " * Word inflection and lemmatization\n",
      " * Parsing\n",
      " * n-grams\n",
      " * Part-of-speech Tagging\n",
      "* Recommended readings:\n",
      " * [Chapter 5, from Natural Language Processing with Python](http://www.nltk.org/book3/).\n",
      "* Activity examples:\n",
      " * Write a program to classify contexts involving the word must according to the tag of the following word. Can this be used to discriminate between the epistemic and deontic uses of must?\n",
      " * Generate some statistics for tagged data in the Brown Corpus to answer the following questions:\n",
      "  - What proportion of word types are always assigned the same part-of-speech tag?\n",
      "  - How many words are ambiguous, in the sense that they appear with at least two tags?\n",
      "  - What percentage of word tokens involve these ambiguous words?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 15"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics\n",
      " * Information extraction\n",
      " * Trees\n",
      " * Named Entity Recognition\n",
      " * Mining HTML, Twitter and Facebook\n",
      "* Recommended readings:\n",
      " * [Chapter 7, from Natural Language Processing with Python](http://www.nltk.org/book3/).\n",
      " * [Chapters 1 and 5, from Mining the Social Web](https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition).\n",
      "* Activity example:\n",
      " * Read the article *[The Automatic Creation of Literature Abstracts](https://text-analysis.googlecode.com/files/luhn58.pdf)* by H.P. Luhn, and develop an approach to rank sentences and generate summaries for home page news from [The New York Times](http://www.nytimes.com/services/xml/rss/index.html)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 16"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics:\n",
      " * Sentiment Analysis\n",
      " * Classifiers\n",
      " * Generative Writing\n",
      "* Recommended readings:\n",
      " * [Chapters 6-7, from Natural Language Processing with Python](http://www.nltk.org/book3/).\n",
      "* Activity example:\n",
      " * Word features can be very useful for performing document classification, since the words that appear in a document give a strong indication about what its semantic content is. However, many words occur very infrequently, and some of the most informative words in a document may never have occurred in our training data. One solution is to make use of a **lexicon**, which describes how different words relate to one another. Using WordNet lexicon, augment the movie review document classifier presented in this chapter to use features that generalize the words that appear in a document, making it more likely that they will match words found in the training data."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 17"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Work on Assignment 2"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Block 4: Network Science"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "[Class 17](../Class17.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics:\n",
      " * Networks\n",
      " * Vertices vs nodes\n",
      " * Graph Theory\n",
      " * NetworkX\n",
      " * Network Analysis\n",
      "* Deadlines:\n",
      " * Assignment 2.\n",
      "* Recommended readings:\n",
      " * [Sections 2.1-2.3, from Networks, Crowds, and Markets: Reasoning About a Highly Connected World](https://www.cs.cornell.edu/home/kleinber/networks-book/networks-book-ch02.pdf).\n",
      " * [Chapter 3](http://www.cs.berkeley.edu/~vazirani/algorithms/chap3.pdf) and [4](http://www.cs.berkeley.edu/~vazirani/algorithms/chap4.pdf), from Algorithms.\n",
      "* Activity example:\n",
      " * Write two functions, `max_degree(graph)` and `min_degree(graph)`, that take a graph and return the maximum and minimum degree of the graph."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "[Class 18](../Class18.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics:\n",
      " * Centrality\n",
      " * Degree\n",
      " * Betweenness\n",
      " * Closeness\n",
      " * Eigenvector\n",
      " * Current flow betweenness\n",
      " * Ego networks\n",
      "* Recommended readings:\n",
      " * [Chapter 3, from Networks, Crowds, and Markets: Reasoning About a Highly Connected World](https://www.cs.cornell.edu/home/kleinber/networks-book/networks-book-ch03.pdf).\n",
      "* Activity example:\n",
      " * Write a function, `centrality_scatter(cent1, cent1)`, that receives two centrality dictionaries and plot each node label as a point using each of dictionary as one of the axes. Add a lienar best-fit trend, axis and title labels."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 19"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics:\n",
      " * Random vs Scale Free\n",
      " * Small Worlds\n",
      " * Network Dynamics\n",
      " * Social Network Analysis\n",
      " * Modularity and Community Structure\n",
      "* Recommended readings:\n",
      " * [Python for Social Science](http://www-rohan.sdsu.edu/~gawron/python_for_ss/course_core/book_draft/Social_Networks/Social_Networks.html#social-networks).\n",
      "* Activity example:\n",
      " * ..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 20"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics:\n",
      " * Multi-partite networks\n",
      " * Meta-netoworks\n",
      " * The Property Graph\n",
      "* Recommended readings:\n",
      " * ...\n",
      "* Activity example:\n",
      " * ..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 21"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Topics:\n",
      " * Network Content Analysis\n",
      " * Plot Analysis\n",
      "* Deadlines:\n",
      " * Final Project proposal.\n",
      "* Recommended readings:\n",
      " * ...\n",
      "* Activity example:\n",
      " * ..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 22"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Work on Assignment 3."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Block 5: Final Projects"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 23"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Work on Final Projects.\n",
      "* Deadlines:\n",
      "  * Assignment 3."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Class 24 and 25"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Final Project presentations"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}